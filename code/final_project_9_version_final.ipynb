{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "corporate-defensive",
   "metadata": {
    "id": "corporate-defensive"
   },
   "source": [
    "# Import libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-sessions",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1622683405896,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "contemporary-sessions",
    "outputId": "660dc373-a88a-4d17-f0f5-8a0ebe317fdc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# data viz libraries\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\") # to make charts look better\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for functions\n",
    "from tqdm import tqdm\n",
    "\n",
    "# for ML\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import plotly modules\n",
    "import chart_studio.plotly as py\n",
    "import cufflinks as cf\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# make it work on jupyter notebook\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# use Plotly locally\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-covering",
   "metadata": {
    "executionInfo": {
     "elapsed": 960,
     "status": "ok",
     "timestamp": 1622683406841,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "backed-covering",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load datasets\n",
    "\n",
    "dataFeatures = pd.read_csv(\"C:/Users/digit/Desktop/Ironhack/project-week-9-final-project/data/features.csv\")\n",
    "dataStores = pd.read_csv(\"C:/Users/digit/Desktop/Ironhack/project-week-9-final-project/data/stores.csv\")\n",
    "dataTest = pd.read_csv(\"C:/Users/digit/Desktop/Ironhack/project-week-9-final-project/data/test.csv\")\n",
    "dataTrain = pd.read_csv(\"C:/Users/digit/Desktop/Ironhack/project-week-9-final-project/data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-memorial",
   "metadata": {
    "id": "regulated-memorial"
   },
   "source": [
    "# EDA and Data Cleaning\n",
    "\n",
    "- Here we will explore the data in order to search for patterns, relationships and to understand the data better. \n",
    "- Perform data cleaning if neccessary and data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-mountain",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1622683406851,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "russian-mountain",
    "outputId": "bd1a01b5-433e-410e-a6c7-1a93d4b209ea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataStores.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-rescue",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1622683406854,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "bacterial-rescue",
    "outputId": "9896fef7-0bd9-487c-8702-aad3a156ed09",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataStores.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-aviation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1622683406856,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "weird-aviation",
    "outputId": "a237e5bf-2b97-48e9-b532-4f1bcf4752c8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFeatures.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-antibody",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1622683406857,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "canadian-antibody",
    "outputId": "3e4b4042-afd6-4bef-9205-ac036220aeea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataFeatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-spine",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1622683406858,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "driving-spine",
    "outputId": "b61033c3-105a-4643-fc19-174b35050a5a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we will start by merging dataStores and dataFeatures since Features is the extension of Stores\n",
    "FeatSto = dataFeatures.merge(dataStores, how=\"inner\", on=\"Store\")\n",
    "\n",
    "# check the head of the new df\n",
    "FeatSto.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-small",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1622683406859,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "chicken-small",
    "outputId": "50df074d-031d-4d4a-ff96-bcbaaad3a102",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FeatSto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-geneva",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1622683406860,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "mediterranean-geneva",
    "outputId": "f748de24-a2a7-4c6b-f7d5-349d3b31da15",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check the dtypes in FeatSto\n",
    "\n",
    "FeatSto.dtypes\n",
    "\n",
    "# Type is of categorical nature\n",
    "# IsHoliday of binary categorical nature \n",
    "\n",
    "# the rest are numerical (Store and Size of discrete type, and the rest of continous type)\n",
    "# some of the features might contain numerical values but still behave as categorical\n",
    "# Date is string and we will convert it into datetime later or drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-ordinary",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1622683407265,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "reported-ordinary",
    "outputId": "a1294dd3-b531-497d-817b-894a4983023b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check for missing values\n",
    "FeatSto.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-monkey",
   "metadata": {
    "id": "flexible-monkey"
   },
   "source": [
    "## Inspect the train and test data (dataTrain and dataTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-bosnia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1622683407268,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "likely-bosnia",
    "outputId": "5581bae8-e501-4357-bd2d-75442bff8e4f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataTest.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-forwarding",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1622683407270,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "massive-forwarding",
    "outputId": "0c782de2-af1f-4d5b-a477-730491bcf4db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-staff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1622683407271,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "natural-staff",
    "outputId": "e93bd00a-1288-4b7e-e18c-64c82cf4444c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# as we can see  dataTrain includes additional Weekly_Sales\n",
    "dataTrain.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-biodiversity",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1622683407272,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "national-biodiversity",
    "outputId": "c44a3293-67fe-401f-d102-18ae7d2fc03c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-piano",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1622683407273,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "declared-piano",
    "outputId": "5c20e880-5a5c-4f42-8dc3-9dc316610dba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTest.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-uniform",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105,
     "status": "ok",
     "timestamp": 1622683407273,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "automated-uniform",
    "outputId": "c708273e-9b7b-4cf4-f0c6-b6f0f171fa53",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataTrain.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-immune",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 75,
     "status": "ok",
     "timestamp": 1622683407274,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "wooden-immune",
    "outputId": "49f8dc97-19ab-44b2-f255-3bd789f8249d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we will disregard the dataTest and use only dataTrain\n",
    "# (we will train-test split the data later)\n",
    "# merge dataTrain with Featsto ->dfwTrain\n",
    "# now we have a dataframe containing dataTrain\n",
    "# FeatSto with dataTrain\n",
    "\n",
    "dfwTrain = pd.merge(FeatSto, dataTrain, how=\"inner\", on=[\"Store\", \"Date\", \"IsHoliday\"])\n",
    "\n",
    "dfwTrain.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-professor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1622683407275,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "revolutionary-professor",
    "outputId": "e0140af2-89cf-4bcb-b6da-9488e19d578a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfwTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-metallic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1622683407660,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "outstanding-metallic",
    "outputId": "c6e34711-a5b1-4c3b-a026-4b0276d36e42",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfwTrain.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-buddy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 103,
     "status": "ok",
     "timestamp": 1622683407662,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "based-buddy",
    "outputId": "eaf93086-8eab-475b-b4f2-a743f8c06bd0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rename dfwTrain into df_total\n",
    "\n",
    "df_total = dfwTrain\n",
    "\n",
    "# show the head of the total dataframe\n",
    "\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-furniture",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1622683407663,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "spare-furniture",
    "outputId": "60039e2a-7206-4922-b17e-d01c9feb85e8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the tail\n",
    "\n",
    "df_total.tail()\n",
    "\n",
    "# first impression:\n",
    "# Date is the week\n",
    "# Markdowns 1 - 5 contain a lot of missing values\n",
    "# Weekly_Sales is numerical continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-neighborhood",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1622683407664,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "removed-neighborhood",
    "outputId": "16dd6950-9322-42db-c570-c31cbd68f58f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the shape of df_total\n",
    "\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-handy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1622683407665,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "extreme-handy",
    "outputId": "fb5f8993-84c0-41f0-ca7e-335e2c973420",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the dtypes of df_total\n",
    "\n",
    "df_total.dtypes\n",
    "\n",
    "# most features have numerical values\n",
    "# Date, Type is a string\n",
    "# some features with numerical values might behave as categoricals, encode them later\n",
    "# such as Type, IsHoliday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-award",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-value",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1622683407666,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "hundred-value",
    "outputId": "c2efae95-df1a-41c8-d154-3eb4f37bbd43",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we can check for missing values\n",
    "\n",
    "df_total.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-paragraph",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1622683407667,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "parental-paragraph",
    "outputId": "36c1e827-24de-4d6e-fa35-6c2e04e4cfbd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculate the percentage of missing values in each column\n",
    "\n",
    "df_total.isnull().sum() / len(df_total)\n",
    "\n",
    "# if the column contains 85% missing values then it should be dropped\n",
    "# MarkDown1-5 contains anonymized data and lots of missing values, despite that, they contain important data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-tragedy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let's create a copy before data manipulation\n",
    "df3_total = df_total.copy(deep=True)\n",
    "df3_total.to_csv(\"df_total.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-protein",
   "metadata": {},
   "source": [
    "### Impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-apparatus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing SimpleImputer from sklearn - this will be used to impute data in the cells with missing values\n",
    "# from https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# missing values - numeric - impute with mean in column \"MarkDown1\"\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer = mean_imputer.fit(df_total[['MarkDown1']])\n",
    "df_total['MarkDown1'] = mean_imputer.transform(df_total[['MarkDown1']]).ravel()\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer = mean_imputer.fit(df_total[['MarkDown2']])\n",
    "df_total['MarkDown2'] = mean_imputer.transform(df_total[['MarkDown2']]).ravel()\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer = mean_imputer.fit(df_total[['MarkDown3']])\n",
    "df_total['MarkDown3'] = mean_imputer.transform(df_total[['MarkDown3']]).ravel()\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer = mean_imputer.fit(df_total[['MarkDown4']])\n",
    "df_total['MarkDown4'] = mean_imputer.transform(df_total[['MarkDown4']]).ravel()\n",
    "\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer = mean_imputer.fit(df_total[['MarkDown5']])\n",
    "df_total['MarkDown5'] = mean_imputer.transform(df_total[['MarkDown5']]).ravel()\n",
    "\n",
    "\n",
    "df_total.to_csv('df_total_imputed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-selection",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1622683408071,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "satisfactory-selection",
    "outputId": "42dc7f16-fd88-40df-e264-775b8b50244b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check for missing values again\n",
    "\n",
    "df_total.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-journal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1622683408072,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "enclosed-journal",
    "outputId": "34c9a86a-f7a5-4557-92cf-1749bdeac204",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for duplicated values\n",
    "\n",
    "df_total.duplicated().sum()\n",
    "\n",
    "# no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-canal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1622683408346,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "demanding-canal",
    "outputId": "35ac18cf-a7cc-4fbc-c477-6409fdb260a0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a Month column\n",
    "\n",
    "df_total[\"Month\"] = pd.to_datetime(df_total['Date']).dt.month\n",
    "df_total.sample(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-thomas",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1622683408348,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "average-thomas",
    "outputId": "9a360fa1-16af-4366-bfea-75736c82279d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a Week column \n",
    "df_total[\"Week\"] = pd.to_datetime(df_total[\"Date\"]).dt.week\n",
    "df_total.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-nickel",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1622683408731,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "computational-nickel",
    "outputId": "2489d02a-84b3-4c27-db11-fa4b197ecec5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# add Year column\n",
    "df_total[\"Year\"] = pd.to_datetime(df_total[\"Date\"]).dt.year \n",
    "df_total.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-lodge",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1622683408732,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "charged-lodge",
    "outputId": "9a6d52a1-e1ed-4f9d-8e00-885adba12cde",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# convert \"Date\" column to datetime format\n",
    "df_total[\"Date\"] = pd.to_datetime(df_total[\"Date\"])\n",
    "df_total.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-kinase",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1622683408733,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "surface-kinase",
    "outputId": "316c63ba-0c32-4937-fade-f6417972ad0c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-forest",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 1175,
     "status": "ok",
     "timestamp": 1622683409895,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "federal-forest",
    "outputId": "051d7420-9622-43e3-9d21-6e5fa172e962",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot Average Monthly Sales - Per Year\n",
    "\n",
    "weekly_sales_2010 = df_total[df_total.Year==2010]['Weekly_Sales'].groupby(df_total['Month']).mean()\n",
    "weekly_sales_2011 = df_total[df_total.Year==2011]['Weekly_Sales'].groupby(df_total['Month']).mean()\n",
    "weekly_sales_2012 = df_total[df_total.Year==2012]['Weekly_Sales'].groupby(df_total['Month']).mean()\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\n",
    "sns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\n",
    "sns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\n",
    "plt.grid()\n",
    "plt.xticks(np.arange(1, 13, step=1))\n",
    "plt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\n",
    "plt.title('Average Monthly Sales - Per Year', fontsize=18)\n",
    "plt.ylabel('Sales', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# 2012 compared to the rest was not doing so well\n",
    "\n",
    "# there is a sharp rise in Sales between January and February, which is connected to SuperBowl\n",
    "# and as we can see, the Monthly Sales are usually spiking in November and December\n",
    "# when Thanksgiving and Christmas are happening\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-transmission",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 33177,
     "status": "ok",
     "timestamp": 1622683443054,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "statistical-transmission",
    "outputId": "32a12ea8-f525-4b0d-f3d5-dd1a462256f3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use Plotly to plot TimeSeries to see whether Date affects Weekly_Sales\n",
    "# make one plot\n",
    "\n",
    "px.line(df_total, x=\"Date\", y=\"Weekly_Sales\", labels={\"x\":\"Date\", \"y\":\"Weekly_Sales\"},\n",
    "       title=\"Weekly Sales across Feb 2010 - Oct 2012\")\n",
    "\n",
    "# in more detail, we can see how Date affects Weekly Sales\n",
    "# the highest spikes are on Thanksgiving Day and Christmas Day\n",
    "# as we have seen in the previous plot, 2012 was not so good in terms of sales for Walmart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-above",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use Plotly to plot a 3-dimensional lineplot\n",
    "# we want to see whether Size has any impact on Weekly Sales\n",
    "fig = px.line_3d(df_total, x='Year', y='Weekly_Sales', z='Size', color='Year')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-presentation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot Average Weekly Sales per Store\n",
    "\n",
    "weekly_sales = df_total[\"Weekly_Sales\"].groupby(df_total[\"Store\"]).mean()\n",
    "fig = px.bar(weekly_sales, y=\"Weekly_Sales\", labels={'Weekly_Sales':'Average Weekly Sales'}, title = \"Average Weekly Sales per Store\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-kruger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot Average Weekly Sales per Department\n",
    "weekly_sales = df_total[\"Weekly_Sales\"].groupby(df_total[\"Dept\"]).mean()\n",
    "fig = px.bar(weekly_sales, y=\"Weekly_Sales\", labels={'Weekly_Sales':'Average Weekly Sales'}, title = \"Average Weekly Sales per Department\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-kennedy",
   "metadata": {},
   "source": [
    "### Stores\n",
    "\n",
    "There are a total of 3 types of stores: Type A, B and C.\n",
    "There are 45 stores in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-lingerie",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sizes=df_total.groupby(\"Store\").Size.count().round(1)\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-maria",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = 'A store','B store','C store'\n",
    "sizes = [(22/(45))*100,(17/(45))*100,(6/(45))*100]\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-yellow",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# boxplot for sizes of types of stores\n",
    "store_type = pd.concat([df_total['Type'], df_total['Size']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x='Type', y='Size', data=store_type)\n",
    "\n",
    "# by boxplot and piechart, we can say that type A store is the largest store and C is the smallest\n",
    "\n",
    "# no overlapping area in size among A,B and C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-dinner",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# boxplot for weekly sales for different types of stores\n",
    "\n",
    "store_sale = pd.concat([df_total['Type'], df_total['Weekly_Sales']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(8, 6))\n",
    "fig = sns.boxplot(x='Type', y='Weekly_Sales', data=store_sale, showfliers=False)\n",
    "\n",
    "# the media of A is the highest and C is the lowest i.e stores with more sizes have higher sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-cookbook",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total count of sales on holidays and non holidays\n",
    "print('sales on non-holiday : ',df_total[df_total['IsHoliday']==False]['Weekly_Sales'].count().round(1))\n",
    "print('sales on holiday : ', df_total[df_total['IsHoliday']==True]['Weekly_Sales'].count().round(1))\n",
    "\n",
    "# surprisingly the sales on Holiday is less than the sales on non-Holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-injection",
   "metadata": {
    "id": "electronic-injection"
   },
   "source": [
    "#### Encode categorical features\n",
    "\n",
    "For Linear Regression, we need to have numerical values. Thus we will encode the categorical features from the dataset into numerical values.\n",
    "\n",
    "We will use One Hot Encoding for \"Type\" since it contains more than 2 levels (A/B/C).\n",
    "\n",
    "\n",
    "Label encode the \"isHoliday\". Since it contains just 2 levels. We use Label Encoder to convert them into model-understandable numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-local",
   "metadata": {
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1622683443066,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "fifteen-local",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-hot-encode \"Type\" categorical variables \n",
    "df_total = pd.get_dummies(df_total, columns=['Type'])\n",
    "\n",
    "# Label-encode \"IsHoliday\" categorical variables \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_total['IsHoliday'] = le.fit_transform(df_total['IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-array",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79,
     "status": "ok",
     "timestamp": 1622683443067,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "engaging-array",
    "outputId": "f221c154-f5d5-49c3-f066-1427b68c1816",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-commission",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-coupon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new features namely - \"IsSuperbowl\", \"IsLaborday\", \"IsThanksgiving\", \"IsChristmas\" \n",
    "def IsSuperbowl(x):\n",
    "  if (x == '2010-02-12') | (x == '2011-02-11') | (x == '2012-02-10') | (x == '2013-02-08'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def IsLaborday(x):\n",
    "  if (x == '2010-09-10') | (x == '2011-09-09') | (x == '2012-09-07') | (x == '2013-09-06'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def IsThanksgiving(x):\n",
    "  if (x == '2010-11-26') | (x == '2011-11-25') | (x == '2012-11-23') | (x == '2013-11-29'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "def IsChristmas(x):\n",
    "  if (x == '2010-12-31') | (x == '2011-12-30') | (x == '2012-12-28') | (x == '2013-12-27'):\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "\n",
    "df_total['IsSuperbowl'] = df_total['Date'].apply(lambda x: IsSuperbowl(x))\n",
    "df_total['IsLaborday'] = df_total['Date'].apply(lambda x: IsLaborday(x))\n",
    "df_total['IsThanksgiving'] = df_total['Date'].apply(lambda x: IsThanksgiving(x))\n",
    "df_total['IsChristmas'] = df_total['Date'].apply(lambda x: IsChristmas(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-aviation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1622683443065,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "musical-aviation",
    "outputId": "9a50c7b1-22dc-42b8-ad37-9545d39797a1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now we can drop Date column now\n",
    "\n",
    "df_total.drop([\"Date\"], inplace=True, axis=1)\n",
    "df_total.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-proportion",
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1622683443068,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "boring-proportion",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# took a sample with the size of 5000, which should be enough to better understand the relationship between the columns\n",
    "\n",
    "# had problems with loading the plot, that's why I saved an image of it\n",
    "\n",
    "# sns.pairplot(df_total.sample(5000), size = 5)\n",
    "\n",
    "#from IPython.display import Image\n",
    "#Image(\"sns_pairplot_df_total.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-tuesday",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 60,
     "status": "ok",
     "timestamp": 1622683443070,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "upset-tuesday",
    "outputId": "5e6309b1-fb4b-401b-fae6-6bbdcc8f79b0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot a scatter matrix in plotly (because sns.pairplot was too heavy)\n",
    "\n",
    "fig = px.scatter_matrix(df_total.sample(1000), dimensions=['Store', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n",
    "       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment',\n",
    "       'IsHoliday', 'Size', 'Dept', 'Weekly_Sales', 'Month', 'Week', 'Year',\n",
    "       'Type_A', 'Type_B', 'Type_C', 'IsSuperbowl', 'IsLaborday',\n",
    "       'IsThanksgiving', 'IsChristmas'], height=5000, width=5000, title=\"Scatter Matrix\", size_max=20)\n",
    "fig.show()\n",
    "\n",
    "# no correlation between features mostly\n",
    "# we can drop Type later\n",
    "# we can drop Size later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-equation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57,
     "status": "ok",
     "timestamp": 1622683443070,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "lasting-equation",
    "outputId": "2c80d03a-7c28-4443-87cd-240c3c887534",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for correlations with correlation matrix\n",
    "corr_matrix = df_total.corr(method=\"pearson\") # we chose 'pearson'\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-chocolate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1622683443071,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "lyric-chocolate",
    "outputId": "2d4ee498-22e5-4af7-edc1-700d5c9b1f25",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot a heatmap for better understanding of correlation\n",
    "fig, ax = plt.subplots(figsize=(18,14))\n",
    "ax = sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# values range between (-1,1)\n",
    "# 0: no correlation at all\n",
    "# 0 - 0.3: weak correlation\n",
    "# 0.3 - 0.7: moderate correlation\n",
    "# 0.7 - 1: strong correlation\n",
    "\n",
    "# strong correlation between MarkDown1 and MarkDown4, drop MarkDown4 later\n",
    "# Year and Fuel_Price show high correlation\n",
    "# Type A store and Size show high correlation\n",
    "# negative correlation between Type A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-cycle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#printing all the correlations to Weekly_Sales in descending order\n",
    "corr_matrix['Weekly_Sales'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-mining",
   "metadata": {
    "id": "increasing-mining"
   },
   "source": [
    "Here we will further inspect the relationship between features and our target variable (\"Weekly_Sales\"), and features that are highly correlated between each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-junction",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8737,
     "status": "ok",
     "timestamp": 1622683451761,
     "user": {
      "displayName": "Tony Ha",
      "photoUrl": "",
      "userId": "06475472458336290099"
     },
     "user_tz": -120
    },
    "id": "variable-junction",
    "outputId": "0f12d186-faad-4304-8b35-4d4bce2753ad",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualise a scatter plot in plotly \n",
    "# to see whether there is correlation between Unemployment and Weekly_Sales\n",
    "# since Walmart is a huge retail company that's competitive thanks to cheap prices\n",
    "# but I think Walmart is a default choice for a lot of people who do not know what they want\n",
    "fig = px.scatter(df_total, x=\"Unemployment\", y=\"Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-rates",
   "metadata": {
    "id": "bound-rates"
   },
   "source": [
    "Is there any correlation between the MarkDown1 -5 and Weekly_Sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-broadcasting",
   "metadata": {
    "id": "plastic-broadcasting",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_total, x=\"MarkDown1\", y=\"Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-roommate",
   "metadata": {
    "id": "premium-roommate",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_total, x=\"MarkDown2\", y=\"Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-carter",
   "metadata": {
    "id": "intense-carter",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_total, x=\"MarkDown3\", y=\"Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-electron",
   "metadata": {
    "id": "minor-electron",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_total, x=\"MarkDown4\", y=\"Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-compatibility",
   "metadata": {
    "id": "protective-compatibility",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_total, x=\"MarkDown5\", y=\"Weekly_Sales\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-radical",
   "metadata": {
    "id": "superior-radical",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df_total, x=\"MarkDown1\", y=\"MarkDown4\")\n",
    "fig.show()\n",
    "\n",
    "# as we can see there is a positive correlation between MarkDown 1 and Markdown4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-repeat",
   "metadata": {},
   "source": [
    "MarkDown 1-5 do not show strong correlation to Weekly_Sales. Fuel_Price shows strong correlation to Year. We will drop Fuel_Price otherwise they would carry similar information to the model. We won't drop Year as it differentiates the same Weeks for Store and Dept.\n",
    "\n",
    "We can analyze other features that have weak with Weekly_Sales to see if they are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-material",
   "metadata": {
    "id": "acting-material",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-meter",
   "metadata": {},
   "source": [
    "## Find outliers\n",
    "\n",
    "Let's check for outliers in the features as LinearRegression is very sensitive to outliers.\n",
    "\n",
    "(We could also see them from the scatter matrix that we plotted earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-combining",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "incorporated-combining",
    "outputId": "f169631a-c47e-4c0a-a640-5652aa146797",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot a histogram to check frequency distribution and spot outliers\n",
    "\n",
    "df_total.hist(figsize=(20,30), xrot=45, bins=40)\n",
    "plt.show()\n",
    "\n",
    "# CPI and Unemployement values are available for certain dates, thus we need to impute them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# missing values - numeric - impute with mean in column \"CPI\"\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer = mean_imputer.fit(df_total[['CPI']])\n",
    "df_total['CPI'] = mean_imputer.transform(df_total[['CPI']]).ravel()\n",
    "# missing values - numeric - impute with mean in column \"Unemployment\"\n",
    "mean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "mean_imputer = mean_imputer.fit(df_total[['Unemployment']])\n",
    "df_total['Unemployment'] = mean_imputer.transform(df_total[['Unemployment']]).ravel()\n",
    "\n",
    "df_total.to_csv('df_total_imputed2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR method for outlier Unemployment\n",
    "# Calculate Q1, Q2 and IQR\n",
    "q1 = df_total['Unemployment'].quantile(0.25)                 \n",
    "q3 = df_total['Unemployment'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "whisker_width = 1.5\n",
    "# Apply filter with respect to IQR, including optional whiskers\n",
    "outlier_unemployment = df_total[(df_total['Unemployment'] < q1 - whisker_width*iqr) | (df_total['Unemployment'] > q3 + whisker_width*iqr)]\n",
    "outlier_unemployment\n",
    "\n",
    "# boxplot with 1.5 whiskers\n",
    "sns.boxplot(y='Unemployment', data = df_total, whis=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-singing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# IQR method for outlier Fuel Priece\n",
    "# Calculate Q1, Q2 and IQR\n",
    "q1 = df_total['CPI'].quantile(0.25)                 \n",
    "q3 = df_total['CPI'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "whisker_width = 1.5\n",
    "# Apply filter with respect to IQR, including optional whiskers\n",
    "outlier_fuel_price = df_total[(df_total['CPI'] < q1 - whisker_width*iqr) | (df_total['CPI'] > q3 + whisker_width*iqr)]\n",
    "outlier_fuel_price\n",
    "\n",
    "# boxplot with 1.5 whiskers\n",
    "sns.boxplot(y='CPI', data = df_total, whis=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat outliers with  Binning\n",
    "# Group the values into certain bins -> e.g Age 0 to 10 in a bin called '0 - 10', etc\n",
    "\n",
    "# Equal width binning -> width = (max value — min value) / N\n",
    "age_range = df_total.Unemployment.max() - df_total.Unemployment.min()\n",
    "min_value = int(np.floor(df_total.Unemployment.min()))\n",
    "max_value = int(np.ceil(df_total.Unemployment.max()))\n",
    " \n",
    "# let's round the bin width\n",
    "# N = number of bins (which is 10 in the below code)\n",
    "# change the value 10 in the below code to see how the grouping differs\n",
    "inter_value = int(np.round(age_range/10))\n",
    " \n",
    "min_value, max_value, inter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-lawyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treat outliers with  Binning\n",
    "# Group the values into certain bins -> e.g Age 0 to 10 in a bin called '0 - 10', etc\n",
    "\n",
    "# Equal width binning -> width = (max value — min value) / N\n",
    "age_range = df_total.CPI.max() - df_total.CPI.min()\n",
    "min_value = int(np.floor(df_total.CPI.min()))\n",
    "max_value = int(np.ceil(df_total.CPI.max()))\n",
    " \n",
    "# let's round the bin width\n",
    "# N = number of bins (which is 10 in the below code)\n",
    "# change the value 10 in the below code to see how the grouping differs\n",
    "inter_value = int(np.round(age_range/10))\n",
    " \n",
    "min_value, max_value, inter_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-third",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr_matrix['Weekly_Sales'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-portuguese",
   "metadata": {
    "id": "peaceful-portuguese",
    "outputId": "f49775b6-6ada-441b-b562-9f1a33b90fb4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.box(df_total, y=\"Temperature\")\n",
    "fig.show()\n",
    "\n",
    "# Temperature has an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-cardiff",
   "metadata": {
    "id": "reasonable-cardiff",
    "outputId": "c13279d3-07f3-4005-d520-064e61adda5f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.box(df_total, y=\"Unemployment\")\n",
    "fig.show()\n",
    "\n",
    "# Unemployment has outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-sculpture",
   "metadata": {
    "id": "great-sculpture",
    "outputId": "15bd2caa-2d10-4f3c-e513-13877fd29f3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.box(df_total, y=\"Fuel_Price\")\n",
    "fig.show()\n",
    "\n",
    "# Fuel Price has no outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-trauma",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-spring",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "Choose the target variable y, and predict from features x. We want to fit a straight line to this data that minimizes the average squared distance between the data sample points and the fitted line. We can use the intercept and slope (which are coefficients) learned from this data to predict y (in this case Weekly Sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review data again to identify which label we want to predict\n",
    "\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to defined the target (dependent) variable we seek to predict\n",
    "# we want to predict Weekly_Sales, isolate the y variable\n",
    "y = df_total[\"Weekly_Sales\"]\n",
    "\n",
    "# then we drop the y variable from the features (X)\n",
    "X = df_total.drop([\"Weekly_Sales\"], axis=1)\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=47)\n",
    "print(f\"Length of train data: {len(X_train)}\")\n",
    "print(f\"Length of test data: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model\n",
    "from sklearn import linear_model\n",
    "\n",
    "# import evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# define the regression model\n",
    "lm = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# next, we fit the model to our data\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "# then calculate a score\n",
    "\n",
    "lm.score(X_train,y_train) # the coefficient of determination R2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-swimming",
   "metadata": {},
   "source": [
    "## Interpret the Coefficients\n",
    "The coefficients(b0 and b1) will allow us to model our equation with values and find the best fit line. The linear_regressor variable (assigned to a LinearRegression object), is able to have the intercept and coefficients extracted, using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints y-intercept\n",
    "print(lm.intercept_)\n",
    "\n",
    "# prints the coefficient\n",
    "print(lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-vulnerability",
   "metadata": {},
   "source": [
    "## Making predictions based on our model\n",
    "Now that we have trained our algorithm, it’s time to make some predictions. To do so, we will use our test data and see how accurately our algorithm predicts the Weekly Sales.\n",
    "\n",
    "Making predictions based on our model, we will use the code below to pass the predict method to our test data. This will return predicted values of target y given the new test X data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have the first imperfect iteration (It1)\n",
    "# y_pred is prediction of y\n",
    "\n",
    "y_pred= lm.predict(X_test) # make predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the training and testing data\n",
    "print(X_test.shape, y_test.shape, X_train.shape, y_train.shape, y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-keeping",
   "metadata": {},
   "source": [
    "## Model Evaluation¶\n",
    "There are three primary metrics used to evaluate linear models. These are: Mean absolute error (MAE), Mean squared error (MSE), or Root mean squared error (RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# print result of MAE\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "#print result of MSE\n",
    "print(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "#print result of RMSE\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-aurora",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The model is not performing well, because we ran a Simple Linear Regression, which operates in 2 dimensions. The dataset is too large and has multiple dimensions.\n",
    "\n",
    "So we will try to run Multiple Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-morning",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-broadway",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "We want to predict Weekly Sales based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-english",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a copy of a df_total\n",
    "\n",
    "df2_total = df_total.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-monkey",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we will drop MarkDown 4 and Fuel_Price \n",
    "df2_total = df2_total.drop([\"MarkDown4\", \"Fuel_Price\"], axis=1)\n",
    "df2_total.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-walnut",
   "metadata": {},
   "source": [
    "## Normalize Data For Comparison\n",
    "\n",
    "We need to scale the data to the range of 0 to 1. We will use MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-rental",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Scale and plot the features against Weekly_Sales (target) using the MinMax scaler (Normalization)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "col_name = df2_total.drop('Weekly_Sales', axis = 1).columns[:]\n",
    "x = df2_total.loc[:, col_name]\n",
    "y = df2_total['Weekly_Sales']\n",
    "\n",
    "# Normalizing x\n",
    "x = pd.DataFrame(data = min_max_scaler.fit_transform(x), columns = col_name)\n",
    "\n",
    "# Examine the normalized data\n",
    "print(df2_total.head())\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-yacht",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run df_total for comparison\n",
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-conservative",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot heatmap to visualize the data after normalisation\n",
    "\n",
    "plt.figure(figsize = (20, 10))\n",
    "sns.heatmap(df2_total.corr(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-disposition",
   "metadata": {},
   "source": [
    "Now we will split the dataset into training set and testing set.\n",
    "We usually use 60 -80 % for training and 20 - 40 % for testing.\n",
    "\n",
    "X is the input dataset to the model\n",
    "y is the output dataset to the model\n",
    "test_size: the percent of data that we want to use for testing, usually from (0.2 - 0.4)\n",
    "random_state: randomly split the train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-casino",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# drop the y variable from the features (X)\n",
    "X = df2_total.drop('Weekly_Sales', axis = 1)\n",
    "# we want to predict Weekly_Sales, isolate the y variable \n",
    "y = df2_total['Weekly_Sales']\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=47)\n",
    "print(\"Train features shape : \", X_train.shape)\n",
    "print(\"Train target shape   : \", y_train.shape)\n",
    "print(\"Test features shape  : \", X_test.shape)\n",
    "print(\"Test target shape    : \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-jacket",
   "metadata": {},
   "source": [
    "## Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-columbia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = LinearRegression(normalize = True) # the parameter normalized = True enables the data to be normalized when fed into the model\n",
    "# fit the training data into the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-advocacy",
   "metadata": {},
   "source": [
    "## Interpret The Model\n",
    "\n",
    "We generated a LinearRegression model that consist of coefficients and intercept. We can now have a look at the intercept and coefficients for our model and interpret them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import metrics library\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-remark",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Model intercept  : \", model.intercept_, \"\\n\")\n",
    "print(\"Model coefficient: \", model.coef_, \"\\n\")\n",
    "\n",
    "for i in range(len(X.columns)):\n",
    "    print(X.columns[i], \": \", model.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-nebraska",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation for training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "rmse = (np.sqrt(mean_squared_error(y_train, y_train_pred)))\n",
    "r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Examine the first 10 predicted output from the model\n",
    "output = pd.DataFrame(y_train[0:15])\n",
    "output['Predicted'] = y_train_pred[0:15]\n",
    "output['Difference'] = output['Predicted'] - output['Weekly_Sales']\n",
    "print(output, \"\\n\")\n",
    "\n",
    "print(\"Model training performance:\")\n",
    "print(\"---------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Model evaluation for testing set\n",
    "y_test_pred = model.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(y_test, y_test_pred)))\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "output = pd.DataFrame(y_test[0:15])\n",
    "output['Predicted'] = y_test_pred[0:15]\n",
    "output['Difference'] = output['Predicted'] - output['Weekly_Sales']\n",
    "print(output, \"\\n\")\n",
    "\n",
    "print(\"Model testing performance:\")\n",
    "print(\"--------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-utilization",
   "metadata": {},
   "source": [
    "The r2 score of our model is just 0.087 and the difference between the actual and predicted value is high.\n",
    "\n",
    "Why is this happening?\n",
    "\n",
    "- The features have no linear relationship with Weekly_Sales.\n",
    "- Features need to be further cleaned and engineered.\n",
    "\n",
    "How to solve it?\n",
    "\n",
    "- Clean the outliers and invalid data.\n",
    "- Try out other models such as DecisionTreeRegressor and GradientBoostingRegressor.\n",
    "\n",
    "- These models can be found in the scikit-learn documentation. With that, we need to examine their r2 score, MSE, RMSE and MAE and compare it with LinearRegression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-virginia",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knn = KNeighborsRegressor(n_neighbors=5,n_jobs=4)\n",
    "knn.fit(X_train,y_train)\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "# MAE: 11108.695624593782\n",
    "# MSE: 18495.403647080002\n",
    "# accuracy = 0.34995246905052524"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-gentleman",
   "metadata": {},
   "source": [
    "# Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt = DecisionTreeRegressor(random_state=0)\n",
    "dt.fit(X_train,y_train)\n",
    "y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "accuracy = dt.score(X_test, y_test)\n",
    "print(accuracy)\n",
    "\n",
    "# MAE: 1955.359370211353\n",
    "# MSE: 5164.17090100692\n",
    "# accuracy = 0.9493220897896165"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-miracle",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor(n_estimators = 400,max_depth=15,n_jobs=5)        \n",
    "rfr.fit(X_train,y_train)\n",
    "y_pred=rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "accuracy = rfr.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "final_project9_version-4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
